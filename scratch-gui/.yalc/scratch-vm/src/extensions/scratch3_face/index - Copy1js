const ArgumentType = require('../../extension-support/argument-type');
const BlockType = require('../../extension-support/block-type');

class Scratch3Face {
  constructor(runtime) {
    this.runtime = runtime;

    this._cameraOn = false;
    this._detections = [];       // [{x,y,w,h, landmarks:[{x,y},...]}]
    this._facesCountCached = 0;

    // Recognition DB: { label: [Float32Array, ...] }
    this._db = Object.create(null);
    this._threshold = 0.85;      // landmark-based embeddings like a slightly higher threshold
    this._targetFPS = 15;

    // MediaPipe
    this._landmarker = null;

    // Overlay flags
    this._drawBoxes = false;
    this._drawLandmarks = false;

    this._lastTick = 0;
	debugger;

    runtime.on('PROJECT_RUN_START', () => this._kickLoop('PROJECT_RUN_START'));
    runtime.on('PROJECT_RUN_STOP',  () => this._emitOverlay(true)); // clear overlay on stop
  }

  // ---------- Overlay toggles ----------
  drawBoxes({ONOFF}) {
    this._drawBoxes = String(ONOFF).toLowerCase() === 'on';
    if (!this._drawBoxes && !this._drawLandmarks) this._emitOverlay(true); // clear
  }
  drawLandmarks({ONOFF}) {
    this._drawLandmarks = String(ONOFF).toLowerCase() === 'on';
    if (!this._drawBoxes && !this._drawLandmarks) this._emitOverlay(true);
  }

  // ---------- LOAD MODEL (MediaPipe Tasks – FaceLandmarker) ----------
  async _ensureModels() {
  console.log("inside ensureModels..");
  console.log(this._landmarker);
    if (this._landmarker) return;

//    const base = this._assetBase(); // 'static/assets/face/'
// //    const tasks = await import(/* webpackIgnore: true */ base + 'tasks-vision.js');
//      console.log('[face] loading tasks from', base + 'tasks-vision.js');
// const tasks = await import(/* webpackIgnore: true */ base + 'tasks-vision.js').catch(e => {
//   console.error('[face] tasks load failed', e);
//   throw e;
// });
 
  const base = this._assetBase();
      console.log('[face] loading tasks from', base + 'tasks-vision.js');

 const tasksUrl = new URL('tasks-vision.js',
   base.startsWith('/') ? window.location.origin + base : window.location.href.replace(/[^/]*$/, '') + base
 ).toString();
 
 let tasks;
 try {
   tasks = await import(/* webpackIgnore: true */ tasksUrl);
 } catch (e) {
   console.error('[face] tasks load failed', e, 'url:', tasksUrl);
   throw e;
 }
 console.log("in landmarks tasks ",tasks);
 const {FilesetResolver, FaceLandmarker} = tasks;
 const wasmBase = base + 'wasm/';
  const files = await FilesetResolver.forVisionTasks(wasmBase);
  
console.log("in landmarks files ");
    // IMPORTANT: point to the wasm directory (with trailing slash)
//    const files = await tasks.FilesetResolver.forVisionTasks(base + 'wasm/');

    // Create Landmarker (no separate detector needed)
    this._landmarker = await tasks.FaceLandmarker.createFromOptions(files, {
     baseOptions: { modelAssetPath: base + 'models/face_landmarker.task' },
//      baseOptions: { modelAssetPath: base + 'models/face_landmarker.task' },
      runningMode: 'IMAGE',
      numFaces: 5, // detect up to 5 faces
      outputFaceBlendshapes: false,
      // Correct spelling per MediaPipe API:
      outputFacialTransformationMatrices: false
    });
     console.log('[face] landmarker ready');
  }

  _assetBase() {
    // RELATIVE path so it works in dev/prod/desktop; must end with slash
    //return 'static/assets/face/';
    const isHttp = /^https?:/i.test(window.location.href);
   // HTTP dev server → absolute path; Electron (file://) → relative
   return isHttp ? '/static/assets/face/' : 'static/assets/face/';
  }

  _getFrame(size = 320) {
    const v = this.runtime.ioDevices.video;
    if (!v) return null;
    return v.getFrame({ format: 'imageData', width: size, height: size }); // ImageData
  }

  // ---------- Detect faces (landmarks -> boxes) ----------
  async _detectOnce() {
  console.log("inside _detectOnce");
      await this._ensureModels();
  console.log("after_ensure models");  

    const frame = this._getFrame(320);
    console.log("after frame");
    if (!frame) {
      console.log('[face] no frame yet (camera not ready?)');
      this._detections = [];
      this._facesCountCached = 0;
      this._emitOverlay(true);
      return;
    }

    if (!this._canvas) {
      this._canvas = document.createElement('canvas');
      this._canvas.width = frame.width;
      this._canvas.height = frame.height;
      this._ctx = this._canvas.getContext('2d');
      console.log('[face] work canvas created', this._canvas.width, this._canvas.height);
    }
    this._ctx.putImageData(frame, 0, 0);

    // Landmarker expects an HTMLCanvasElement for IMAGE mode
    const res = this._landmarker.detect(this._canvas);
    const W = this._canvas.width, H = this._canvas.height;

    const faces = res?.faceLandmarks || [];
    console.log("calling _detections..");
    this._detections = faces.map(lm => {
      // lm points are normalized [0..1]; convert to pixel coords
      let minx = Infinity, miny = Infinity, maxx = -Infinity, maxy = -Infinity;
      const px = lm.map(p => {
        const x = p.x * W, y = p.y * H;
        if (x < minx) minx = x; if (x > maxx) maxx = x;
        if (y < miny) miny = y; if (y > maxy) maxy = y;
        return {x, y};
      });
      return {
        x: Math.round(minx),
        y: Math.round(miny),
        w: Math.round(maxx - minx),
        h: Math.round(maxy - miny),
        landmarks: px // pixel landmarks for overlay & embedding
      };
    });

    this._facesCountCached = this._detections.length;
    console.log('[face] detected faces:', this._facesCountCached);
    this._emitOverlay(); // draw boxes/points if toggled on
  }

  // ---------- Landmark-based "embedding" (no ML model) ----------
  _getKeyIdxs() {
    // Stable subset of MediaPipe FaceMesh points
    return [33,263,133,362,1,4,61,291,13,14,10,152,234,454];
  }

  _embedFromLandmarks(lm) {
    // expects full landmark array (often 468 pts) with {x,y} in pixels
    if (!lm || lm.length < 468) return null;

    const idxs = this._getKeyIdxs();
    const R_outer = lm[33], L_outer = lm[263];
    const cx = (R_outer.x + L_outer.x) / 2;
    const cy = (R_outer.y + L_outer.y) / 2;
    const eyeDist = Math.hypot(L_outer.x - R_outer.x, L_outer.y - R_outer.y) || 1;

    const vec = [];
    for (const i of idxs) {
      const p = lm[i];
      vec.push((p.x - cx) / eyeDist, (p.y - cy) / eyeDist);
    }

    // Add a few invariant distances
    const mouthL = lm[61], mouthR = lm[291];
    const nose  = lm[1],   chin   = lm[152];
    const eyeInR = lm[133], eyeInL = lm[362];

    const mouthW = Math.hypot(mouthR.x - mouthL.x, mouthR.y - mouthL.y) / eyeDist;
    const faceH  = Math.hypot(chin.x - nose.x,     chin.y - nose.y)     / eyeDist;
    const eyeW   = Math.hypot(eyeInL.x - eyeInR.x, eyeInL.y - eyeInR.y) / eyeDist;

    vec.push(mouthW, faceH, eyeW);

    // L2-normalize
    let norm = 0; for (const v of vec) norm += v*v;
    norm = Math.sqrt(norm) || 1;
    for (let i=0; i<vec.length; i++) vec[i] /= norm;

    return new Float32Array(vec);
  }

  // ---------- Tick loop ----------
  _tickLoop() {
    console.log("in _tickLoop...");
    const loop = async () => {
      const now = performance.now();
      const interval = 1000 / this._targetFPS;
      console.log("now - this._lastTick >= interval && this._cameraOn");
      if (now - this._lastTick >= interval && this._cameraOn) {
        try { await this._detectOnce(); } catch (e) { /* swallow */ }
        this._lastTick = now;
      }
      if (this.runtime && this.runtime.started) {
        if (typeof window !== 'undefined') window.requestAnimationFrame(loop);
        else setTimeout(loop, 30);
      }
    };
    loop();
  }
  
  _kickLoop(source) {
  console.log("in _kickLoop...");
   if (this._loopStarted) return;
   this._loopStarted = true;
    console.log('[face] tick loop start from', source);
   this._tickLoop();
 }

  // ---------- Overlay emitter ----------
  _emitOverlay(forceClear = false) {
  console.log(" inside _emitOverlay");
  console.log(forceClear);

    if (!this.runtime || (!this._drawBoxes && !this._drawLandmarks && !forceClear)) return;
    const payload = {
      src: 'face',
      boxes: forceClear ? [] : (this._drawBoxes ? this._detections.map(d => ({x:d.x, y:d.y, w:d.w, h:d.h})) : []),
      landmarks: forceClear ? [] : (this._drawLandmarks ? this._detections.map(d =>
        (d.landmarks || []).map(p => ({x: Math.round(p.x), y: Math.round(p.y)}))
      ) : []),
      frameWidth: this._canvas ? this._canvas.width : 0,
      frameHeight: this._canvas ? this._canvas.height : 0,
      ts: Date.now()
    };
    console.log("at the end of emit");
    console.log('[face] emit overlay', {
   boxes: payload.boxes.length,
   lm: payload.landmarks.length,
   fw: payload.frameWidth,
   fh: payload.frameHeight,
   clear: forceClear
 });
    this.runtime.emit('FACE_OVERLAY', payload);
  }

  // ---------- Scratch extension surface ----------
  getInfo() {
    return {
      id: 'face',
      name: 'Face Blocks',
      color1: '#6A5ACD',
      color2: '#4E3CC6',
      blocks: [
        { opcode: 'startCamera', blockType: BlockType.COMMAND, text: 'start camera' },
        { opcode: 'stopCamera',  blockType: BlockType.COMMAND, text: 'stop camera' },
        { opcode: 'cameraReady', blockType: BlockType.BOOLEAN, text: 'camera ready?' },

        { opcode: 'facesCount',  blockType: BlockType.REPORTER, text: 'faces count' },
        { opcode: 'faceDetected',blockType: BlockType.BOOLEAN,  text: 'face [INDEX] detected?', arguments: { INDEX:{type:ArgumentType.NUMBER, defaultValue:1} } },
        { opcode: 'faceX',       blockType: BlockType.REPORTER, text: 'face [INDEX] x', arguments:{ INDEX:{type:ArgumentType.NUMBER, defaultValue:1} } },
        { opcode: 'faceY',       blockType: BlockType.REPORTER, text: 'face [INDEX] y', arguments:{ INDEX:{type:ArgumentType.NUMBER, defaultValue:1} } },
        { opcode: 'faceW',       blockType: BlockType.REPORTER, text: 'face [INDEX] width', arguments:{ INDEX:{type:ArgumentType.NUMBER, defaultValue:1} } },
        { opcode: 'faceH',       blockType: BlockType.REPORTER, text: 'face [INDEX] height', arguments:{ INDEX:{type:ArgumentType.NUMBER, defaultValue:1} } },

        { opcode: 'drawBoxes',     blockType: BlockType.COMMAND,  text: 'draw boxes [ONOFF]', arguments: { ONOFF:{type:ArgumentType.STRING, defaultValue:'on'} } },
        { opcode: 'drawLandmarks', blockType: BlockType.COMMAND,  text: 'draw landmarks [ONOFF]', arguments: { ONOFF:{type:ArgumentType.STRING, defaultValue:'off'} } },

        { opcode: 'trainFace',   blockType: BlockType.COMMAND,  text: 'train face [INDEX] as [LABEL]', arguments:{
            INDEX:{type:ArgumentType.NUMBER, defaultValue:1}, LABEL:{type:ArgumentType.STRING, defaultValue:'Alice'}
        }},
        { opcode: 'recognizeAs', blockType: BlockType.BOOLEAN,  text: 'recognize face [INDEX] as [LABEL]?', arguments:{
            INDEX:{type:ArgumentType.NUMBER, defaultValue:1}, LABEL:{type:ArgumentType.STRING, defaultValue:'Alice'}
        }}
      ]
    };
  }

  // ---------- Block opcodes ----------
//  startCamera() { this.runtime.ioDevices.video.enableVideo(); this._cameraOn = true; }
   startCamera() {
   console.log("in start camera");
   this.runtime.ioDevices.video.enableVideo();
   this._cameraOn = true;
   this._kickLoop('startCamera'); // <-- ensure loop runs even if no green-flag
 }

  stopCamera()  { this.runtime.ioDevices.video.disableVideo(); this._cameraOn = false; this._emitOverlay(true); }
  cameraReady() { return !!this._cameraOn; }

  facesCount() { return this._facesCountCached || 0; }
  faceDetected({INDEX}) { const i = Math.max(1, Math.floor(INDEX)) - 1; return !!this._detections[i]; }
  faceX({INDEX}) { const d = this._detections[Math.max(1, Math.floor(INDEX)) - 1]; return d ? d.x : 0; }
  faceY({INDEX}) { const d = this._detections[Math.max(1, Math.floor(INDEX)) - 1]; return d ? d.y : 0; }
  faceW({INDEX}) { const d = this._detections[Math.max(1, Math.floor(INDEX)) - 1]; return d ? d.w : 0; }
  faceH({INDEX}) { const d = this._detections[Math.max(1, Math.floor(INDEX)) - 1]; return d ? d.h : 0; }

  async trainFace({INDEX, LABEL}) {
    const d = this._detections[Math.max(1, Math.floor(INDEX)) - 1];
    if (!d || !LABEL) return;
    const emb = this._embedFromLandmarks(d.landmarks);
    if (!emb) return;
    (this._db[LABEL] ||= []).push(emb);
  }

  async recognizeAs({INDEX, LABEL}) {
    const d = this._detections[Math.max(1, Math.floor(INDEX)) - 1];
    const arr = this._db[LABEL];
    if (!d || !arr || arr.length === 0) return false;
    const emb = this._embedFromLandmarks(d.landmarks);
    if (!emb) return false;

    // cosine similarity
    let best = -1;
    for (const v of arr) {
      let dot=0, na=0, nb=0;
      for (let i=0;i<v.length;i++){ dot+=v[i]*emb[i]; na+=v[i]*v[i]; nb+=emb[i]*emb[i]; }
      const cos = dot / (Math.sqrt(na)*Math.sqrt(nb) + 1e-9);
      if (cos > best) best = cos;
    }
    return best >= (this._threshold || 0.85);
  }
}

module.exports = Scratch3Face;
